{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting codecarbon\n",
      "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting arrow (from codecarbon)\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from codecarbon) (8.1.7)\n",
      "Collecting fief-client[cli] (from codecarbon)\n",
      "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from codecarbon) (2.2.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from codecarbon) (0.21.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from codecarbon) (5.9.0)\n",
      "Collecting py-cpuinfo (from codecarbon)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pynvml (from codecarbon)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting questionary (from codecarbon)\n",
      "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting rapidfuzz (from codecarbon)\n",
      "  Downloading rapidfuzz-3.12.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from codecarbon) (2.32.3)\n",
      "Collecting rich (from codecarbon)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting typer (from codecarbon)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from arrow->codecarbon) (2.9.0.post0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
      "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from fief-client[cli]->codecarbon) (0.27.0)\n",
      "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
      "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
      "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from pandas->codecarbon) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from pandas->codecarbon) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from pandas->codecarbon) (2025.1)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->codecarbon)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from questionary->codecarbon) (3.0.43)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from requests->codecarbon) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from requests->codecarbon) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from requests->codecarbon) (2025.1.31)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->codecarbon)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from rich->codecarbon) (2.15.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from typer->codecarbon) (4.12.2)\n",
      "Collecting shellingham>=1.3.0 (from typer->codecarbon)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
      "Collecting cryptography>=3.4 (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->codecarbon)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/ForPyTorch/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.21)\n",
      "Downloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
      "Downloading rapidfuzz-3.12.2-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
      "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
      "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, types-python-dateutil, termcolor, shellingham, rapidfuzz, pynvml, mdurl, yaspin, questionary, markdown-it-py, cryptography, arrow, rich, jwcrypto, typer, fief-client, codecarbon\n",
      "Successfully installed arrow-1.3.0 codecarbon-2.8.3 cryptography-44.0.2 fief-client-0.20.0 jwcrypto-1.5.6 markdown-it-py-3.0.0 mdurl-0.1.2 nvidia-ml-py-12.570.86 py-cpuinfo-9.0.0 pynvml-12.0.0 questionary-2.1.0 rapidfuzz-3.12.2 rich-13.9.4 shellingham-1.5.4 termcolor-2.3.0 typer-0.15.2 types-python-dateutil-2.9.0.20241206 yaspin-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# for padding a list of variable length tensors to equal length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# for packing a tensor containing padded sequences of variable length\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import nltk\n",
    "# for removing stop words like 'the', 'and', etc\n",
    "# stop words don't provide useful information\n",
    "from nltk.corpus import stopwords\n",
    "# for tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# for counting the number of unique tokens\n",
    "from collections import Counter\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# get cpu, gpu or mps device for computation\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  dark_pigmentation  acne  \\\n",
      "0  Australian Gold Deviously Black 45X Dark Bronz...                  0     0   \n",
      "1  Australian Gold Sunscreen  Spray Gel with Inst...                  0     0   \n",
      "2  Australian Gold Sunscreen Spray Gel - SPF 15 -...                  0     0   \n",
      "3  Avalon Organics Intense Defense Oil-Free Moist...                  0     0   \n",
      "4  Avalon Organics Vitamin C Renewal Facial Cream...                  0     0   \n",
      "\n",
      "   eye_contour  homogeneity  lack_firmness  lack_radiance  pores  fine_lines  \\\n",
      "0            0            0              0              0      0           0   \n",
      "1            0            0              0              1      0           0   \n",
      "2            0            0              0              0      0           0   \n",
      "3            0            0              0              0      0           0   \n",
      "4            0            1              0              1      0           0   \n",
      "\n",
      "   wrinkles_fine-lines  ...  female  cleanse  prepare  treat  targeted  care  \\\n",
      "0                    0  ...       1        0        0      0         0     0   \n",
      "1                    0  ...       1        0        0      0         0     0   \n",
      "2                    0  ...       1        0        0      0         0     0   \n",
      "3                    0  ...       1        0        0      0         0     0   \n",
      "4                    1  ...       1        0        0      1         0     0   \n",
      "\n",
      "   moisturize  protect  day  night  \n",
      "0           0        0    1      1  \n",
      "1           0        1    1      0  \n",
      "2           1        1    1      0  \n",
      "3           1        1    1      1  \n",
      "4           1        0    1      1  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the concatenated_MV file as a pandas DataFrame\n",
    "file_path = 'concatenated_MV.xlsx'  # Update this path to the actual file location\n",
    "df_loreal = pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_loreal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of output classes: 33\n",
      "List of output classes:\n",
      "['dark_pigmentation', 'acne', 'eye_contour', 'homogeneity', 'lack_firmness', 'lack_radiance', 'pores', 'fine_lines', 'wrinkles_fine-lines', 'eye-wrinkles', 'undereye-bags', 'generic', '18-34', '35-54', '55-99', 'dry', 'normal', 'oily', 'combination', 'sensitivity-high', 'sensitivity-low', 'no_sensitivity', 'male', 'female', 'cleanse', 'prepare', 'treat', 'targeted', 'care', 'moisturize', 'protect', 'day', 'night']\n"
     ]
    }
   ],
   "source": [
    "# Extract the output class names (column names excluding the text column)\n",
    "output_classes = list(df_loreal.columns[1:34])  # Exclude the 'text_raw' column\n",
    "\n",
    "num_classes = len(output_classes)\n",
    "\n",
    "# Display the output\n",
    "print(f\"Number of output classes: {num_classes}\")\n",
    "print(\"List of output classes:\")\n",
    "print(output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sebastian/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sebastian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sebastian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australian Gold Deviously Black 45X Dark Bronz...</td>\n",
       "      <td>[australian, gold, deviously, black, 45x, dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australian Gold Sunscreen  Spray Gel with Inst...</td>\n",
       "      <td>[australian, gold, sunscreen, spray, gel, inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australian Gold Sunscreen Spray Gel - SPF 15 -...</td>\n",
       "      <td>[australian, gold, sunscreen, spray, gel, -, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avalon Organics Intense Defense Oil-Free Moist...</td>\n",
       "      <td>[avalon, organics, intense, defense, oil-free,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avalon Organics Vitamin C Renewal Facial Cream...</td>\n",
       "      <td>[avalon, organics, vitamin, c, renewal, facial...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Australian Gold Deviously Black 45X Dark Bronz...   \n",
       "1  Australian Gold Sunscreen  Spray Gel with Inst...   \n",
       "2  Australian Gold Sunscreen Spray Gel - SPF 15 -...   \n",
       "3  Avalon Organics Intense Defense Oil-Free Moist...   \n",
       "4  Avalon Organics Vitamin C Renewal Facial Cream...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [australian, gold, deviously, black, 45x, dark...  \n",
       "1  [australian, gold, sunscreen, spray, gel, inst...  \n",
       "2  [australian, gold, sunscreen, spray, gel, -, s...  \n",
       "3  [avalon, organics, intense, defense, oil-free,...  \n",
       "4  [avalon, organics, vitamin, c, renewal, facial...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# set the stop words to the English mode\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# define a function for pre-processing\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # remove stop words\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the 'text_raw' column\n",
    "df_loreal[\"tokens\"] = df_loreal[\"description\"].apply(preprocess_text)\n",
    "\n",
    "# Display first few tokenized samples\n",
    "df_loreal[[\"description\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique tokens in the vocabulary: 24064\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "all_tokens = [token for tokens in df_loreal['tokens'] for token in tokens]\n",
    "token_counts = Counter(all_tokens)\n",
    "print('The number of unique tokens in the vocabulary:', len(token_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Normal Distribution: mu = 25.94, sigma = 392.86\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Assume token_counts is already defined from our vocabulary creation step:\n",
    "# token_counts = Counter(all_tokens)\n",
    "\n",
    "# Get an array of token frequencies\n",
    "frequencies = np.array(list(token_counts.values()))\n",
    "\n",
    "# Plot the histogram of token frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(frequencies, bins=50, density=True, alpha=0.6, color='skyblue', edgecolor='black', label='Token Frequency Histogram')\n",
    "\n",
    "# Fit a normal distribution to the frequency data\n",
    "mu, sigma = norm.fit(frequencies)\n",
    "print(f\"Fitted Normal Distribution: mu = {mu:.2f}, sigma = {sigma:.2f}\")\n",
    "\n",
    "# Set the x-axis maximum to 10,000\n",
    "xmin, _ = plt.xlim()\n",
    "xmax = 5000\n",
    "plt.xlim(xmin, xmax)\n",
    "\n",
    "# Create a range of values for plotting the PDF of the fitted normal distribution\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# Plot the fitted normal distribution curve\n",
    "plt.plot(x, p, 'r', linewidth=2, label=f'Normal fit (mu={mu:.2f}, sigma={sigma:.2f})')\n",
    "plt.xlabel('Token Frequency')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Token Frequency Distribution with Fitted Normal Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "      <th>encoded_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australian Gold Deviously Black 45X Dark Bronz...</td>\n",
       "      <td>[australian, gold, deviously, black, 45x, dark...</td>\n",
       "      <td>[1002, 390, 9460, 384, 12863, 54, 1687, 4400, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australian Gold Sunscreen  Spray Gel with Inst...</td>\n",
       "      <td>[australian, gold, sunscreen, spray, gel, inst...</td>\n",
       "      <td>[1002, 390, 29, 173, 61, 270, 1687, 9, 26, 213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australian Gold Sunscreen Spray Gel - SPF 15 -...</td>\n",
       "      <td>[australian, gold, sunscreen, spray, gel, -, s...</td>\n",
       "      <td>[1002, 390, 29, 173, 61, 9, 26, 126, 9, 392, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avalon Organics Intense Defense Oil-Free Moist...</td>\n",
       "      <td>[avalon, organics, intense, defense, oil-free,...</td>\n",
       "      <td>[3066, 1539, 397, 541, 156, 20, 2, 100, 17, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avalon Organics Vitamin C Renewal Facial Cream...</td>\n",
       "      <td>[avalon, organics, vitamin, c, renewal, facial...</td>\n",
       "      <td>[3066, 1539, 24, 53, 367, 31, 7, 2, 3432, 3066...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  Australian Gold Deviously Black 45X Dark Bronz...   \n",
       "1  Australian Gold Sunscreen  Spray Gel with Inst...   \n",
       "2  Australian Gold Sunscreen Spray Gel - SPF 15 -...   \n",
       "3  Avalon Organics Intense Defense Oil-Free Moist...   \n",
       "4  Avalon Organics Vitamin C Renewal Facial Cream...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [australian, gold, deviously, black, 45x, dark...   \n",
       "1  [australian, gold, sunscreen, spray, gel, inst...   \n",
       "2  [australian, gold, sunscreen, spray, gel, -, s...   \n",
       "3  [avalon, organics, intense, defense, oil-free,...   \n",
       "4  [avalon, organics, vitamin, c, renewal, facial...   \n",
       "\n",
       "                                      encoded_tokens  \n",
       "0  [1002, 390, 9460, 384, 12863, 54, 1687, 4400, ...  \n",
       "1  [1002, 390, 29, 173, 61, 270, 1687, 9, 26, 213...  \n",
       "2  [1002, 390, 29, 173, 61, 9, 26, 126, 9, 392, 8...  \n",
       "3  [3066, 1539, 397, 541, 156, 20, 2, 100, 17, 30...  \n",
       "4  [3066, 1539, 24, 53, 367, 31, 7, 2, 3432, 3066...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set vocabulary size (excluding special tokens)\n",
    "vocab_size = 24000  # Adjust as needed\n",
    "\n",
    "# Keep only the most common words (excluding special tokens)\n",
    "common_tokens = token_counts.most_common(vocab_size - 2)  # Reserve space for <PAD> and <UNK>\n",
    "\n",
    "# Create token-to-index mappings\n",
    "token_to_idx = {token: idx + 2 for idx, (token, _) in enumerate(common_tokens)}\n",
    "\n",
    "# Special tokens\n",
    "token_to_idx[\"<PAD>\"] = 0  # Padding token\n",
    "token_to_idx[\"<UNK>\"] = 1  # Unknown token\n",
    "\n",
    "# Create index-to-token mapping (for decoding)\n",
    "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
    "\n",
    "# Define a function to encode tokens into integer sequences\n",
    "def encode_tokens(tokens):\n",
    "    return [token_to_idx.get(token, token_to_idx[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# Apply encoding to the dataset\n",
    "df_loreal[\"encoded_tokens\"] = df_loreal[\"tokens\"].apply(encode_tokens)\n",
    "\n",
    "# Display first few rows with encoded tokens\n",
    "df_loreal[[\"description\", \"tokens\", \"encoded_tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset class\n",
    "class LOREALMultiLabelDataset(Dataset):\n",
    "    def __init__(self, encoded_tokens_list, labels):\n",
    "        self.encoded_tokens_list = encoded_tokens_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.encoded_tokens_list[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.float)  # Multi-label classification requires float\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define a collate function to handle variable sequence lengths\n",
    "def collate_batch(batch):\n",
    "    batch_X, batch_y, batch_length = [], [], []\n",
    "\n",
    "    for (X, y) in batch:\n",
    "        batch_X.append(X)\n",
    "        batch_y.append(y)\n",
    "        batch_length.append(len(X))\n",
    "\n",
    "    # Convert batch_length to tensor\n",
    "    batch_length = torch.tensor(batch_length, dtype=torch.int64)\n",
    "    batch_y = torch.stack(batch_y)  # Stack labels into a tensor\n",
    "\n",
    "    # Pad sequences to equal length\n",
    "    batch_X = pad_sequence(batch_X, batch_first=True)\n",
    "\n",
    "    return batch_X, batch_y, batch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a single dataset instance from your encoded tokens and multi-label targets\n",
    "ds = LOREALMultiLabelDataset(\n",
    "    df_loreal[\"encoded_tokens\"].tolist(), \n",
    "    df_loreal.iloc[:, 1:34].values  # columns 2 to 33 for labels\n",
    ")\n",
    "train_val_ds, test_ds = random_split(ds, [0.7, 0.3])\n",
    "train_ds, val_ds = random_split(train_val_ds, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3495\n",
      "Validation dataset size: 873\n",
      "Test dataset size: 1872\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Train dataset size: {len(train_ds)}\")\n",
    "print(f\"Validation dataset size: {len(val_ds)}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class MultiLabelBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(MultiLabelBiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Bidirectional LSTM: hidden size is doubled after concatenation\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed_x)\n",
    "        # For a single layer bidirectional LSTM, h[-2] is from the forward and h[-1] from the backward pass.\n",
    "        h_forward = h[-2]\n",
    "        h_backward = h[-1]\n",
    "        h_combined = torch.cat((h_forward, h_backward), dim=1)\n",
    "        return torch.sigmoid(self.fc(h_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelBiLSTM(\n",
      "  (embedding): Embedding(24000, 128)\n",
      "  (lstm): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=33, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "embed_size = 128\n",
    "hidden_size = 64\n",
    "\n",
    "# Create the model instance\n",
    "model = MultiLabelBiLSTM(vocab_size, embed_size, hidden_size, num_classes)\n",
    "\n",
    "# Move model to computation device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def train(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    early_stopping_patience,\n",
    "    lr_scheduler,\n",
    "    saved_path_prefix\n",
    "):\n",
    "    # initialization\n",
    "    min_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    histories = {\n",
    "        'train_batch': [],\n",
    "        'train_epoch': [],\n",
    "        'val_batch': [],\n",
    "        'val_epoch': []\n",
    "    }\n",
    "    saved_path = ''\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(epochs):\n",
    "        # train set\n",
    "        train_epoch_loss = 0.0\n",
    "        model.train()\n",
    "        for X, y, lens in tqdm(train_dl, desc = f'Training batch\\t'): # tqdm progress bar\n",
    "            X, y, lens = X.to(device), y.to(device), lens.to(device)\n",
    "            logits = model(X, lens)\n",
    "            train_batch_loss = loss_fn(logits, y)\n",
    "            train_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            histories['train_batch'].append(train_batch_loss.item())\n",
    "            train_epoch_loss += train_batch_loss.item()\n",
    "        train_epoch_loss /= len(train_dl)\n",
    "        histories['train_epoch'].append(train_epoch_loss)\n",
    "\n",
    "        # validation set\n",
    "        val_epoch_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y, lens in tqdm(val_dl, desc = f'Validation batch'): # tqdm progress bar\n",
    "                X, y, lens = X.to(device), y.to(device), lens.to(device)\n",
    "                logits = model(X, lens)\n",
    "                val_batch_loss = loss_fn(logits, y)\n",
    "                histories['val_batch'].append(val_batch_loss.item())\n",
    "                val_epoch_loss += val_batch_loss.item()\n",
    "            val_epoch_loss /= len(val_dl)\n",
    "            histories['val_epoch'].append(val_epoch_loss)\n",
    "\n",
    "        # print log\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_consumed = str(end_time - start_time).split('.')[0]\n",
    "        print(f\"Epoch {epoch + 1}: train loss = {train_epoch_loss:>5f}, val loss = {val_epoch_loss:>5f}, time consumed = {time_consumed}\")\n",
    "\n",
    "        # learning rate decay\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_scheduler.step(val_epoch_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if current_lr != new_lr:\n",
    "            print(f'Learning rate reduced after epoch {epoch+1}\\n')\n",
    "\n",
    "        # early stopping\n",
    "        if val_epoch_loss < min_val_loss:\n",
    "            if os.path.exists(saved_path):\n",
    "                os.remove(saved_path)\n",
    "            time_str = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            saved_path = saved_path_prefix + f'epoch{epoch+1}val_loss{val_epoch_loss:>4f}_{time_str}.pth'\n",
    "            torch.save(model.state_dict(), saved_path)\n",
    "            print(f'Model saved after epoch {epoch+1}')\n",
    "            patience_counter += 1\n",
    "            if (val_epoch_loss + 0.00009) < min_val_loss:\n",
    "                patience_counter = 0\n",
    "                print('Reset patience counter\\n')            \n",
    "            min_val_loss = val_epoch_loss\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "    return histories, saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training hyper-parameters\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-1\n",
    "weight_decay = 1e-5\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "epochs = 10000000\n",
    "early_stopping_patience = 5\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode = 'min',\n",
    "    factor = 0.1,\n",
    "    patience = 2\n",
    ")\n",
    "saved_path_prefix = 'loreal_product_classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 22:08:45] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 22:08:45] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 22:08:45] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 22:08:45] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Mac OS and ARM processor detected: Please enable PowerMetrics sudo to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 22:08:45] We saw that you have a Apple M3 Pro but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 22:08:45] CPU Model on constant consumption mode: Apple M3 Pro\n",
      "[codecarbon INFO @ 22:08:45] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 22:08:45] No GPU found.\n",
      "[codecarbon INFO @ 22:08:45] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 22:08:45]   Platform system: macOS-14.6.1-arm64-arm-64bit\n",
      "[codecarbon INFO @ 22:08:45]   Python version: 3.12.8\n",
      "[codecarbon INFO @ 22:08:45]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 22:08:45]   Available RAM : 36.000 GB\n",
      "[codecarbon INFO @ 22:08:45]   CPU count: 11\n",
      "[codecarbon INFO @ 22:08:45]   CPU model: Apple M3 Pro\n",
      "[codecarbon INFO @ 22:08:45]   GPU count: None\n",
      "[codecarbon INFO @ 22:08:45]   GPU model: None\n",
      "[codecarbon INFO @ 22:08:48] Saving emissions data to file /Users/sebastian/Documents/Jup for run/emissions.csv\n"
     ]
    }
   ],
   "source": [
    "tracker = EmissionsTracker(output_file=\"emissions.csv\", log_level=\"info\", allow_multiple_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.06it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.756172, val loss = 0.756357, time consumed = 0:00:13\n",
      "Model saved after epoch 1\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   4%|▎         | 1/28 [00:00<00:17,  1.52it/s][codecarbon INFO @ 22:09:08] Energy consumed for RAM : 0.000056 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:09:08] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:09:08] 0.000233 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:14<00:00,  1.88it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 22.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train loss = 0.753993, val loss = 0.754056, time consumed = 0:00:29\n",
      "Model saved after epoch 2\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   4%|▎         | 1/28 [00:00<00:16,  1.62it/s][codecarbon INFO @ 22:09:23] Energy consumed for RAM : 0.000113 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:09:23] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:09:23] 0.000467 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:14<00:00,  1.99it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train loss = 0.751718, val loss = 0.751695, time consumed = 0:00:43\n",
      "Model saved after epoch 3\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  11%|█         | 3/28 [00:01<00:11,  2.09it/s][codecarbon INFO @ 22:09:38] Energy consumed for RAM : 0.000169 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:09:38] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:09:38] 0.000700 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.02it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train loss = 0.749230, val loss = 0.749248, time consumed = 0:00:57\n",
      "Model saved after epoch 4\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  14%|█▍        | 4/28 [00:02<00:13,  1.82it/s][codecarbon INFO @ 22:09:53] Energy consumed for RAM : 0.000225 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:09:53] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:09:53] 0.000934 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train loss = 0.746718, val loss = 0.746701, time consumed = 0:01:10\n",
      "Model saved after epoch 5\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  36%|███▌      | 10/28 [00:04<00:08,  2.24it/s][codecarbon INFO @ 22:10:08] Energy consumed for RAM : 0.000281 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:10:08] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:10:08] 0.001167 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train loss = 0.743972, val loss = 0.744066, time consumed = 0:01:23\n",
      "Model saved after epoch 6\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  54%|█████▎    | 15/28 [00:06<00:05,  2.37it/s][codecarbon INFO @ 22:10:23] Energy consumed for RAM : 0.000338 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:10:23] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:10:23] 0.001400 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.31it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train loss = 0.741584, val loss = 0.741314, time consumed = 0:01:35\n",
      "Model saved after epoch 7\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:09<00:03,  1.98it/s][codecarbon INFO @ 22:10:38] Energy consumed for RAM : 0.000394 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:10:38] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:10:38] 0.001634 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.33it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train loss = 0.739071, val loss = 0.738475, time consumed = 0:01:48\n",
      "Model saved after epoch 8\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  93%|█████████▎| 26/28 [00:11<00:00,  2.42it/s][codecarbon INFO @ 22:10:53] Energy consumed for RAM : 0.000450 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:10:53] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:10:53] 0.001867 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:10:53] 0.000872 g.CO2eq/s mean an estimation of 27.488741354486237 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.30it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: train loss = 0.735431, val loss = 0.735534, time consumed = 0:02:00\n",
      "Model saved after epoch 9\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.30it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train loss = 0.732822, val loss = 0.732517, time consumed = 0:02:12\n",
      "Model saved after epoch 10\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:09,  2.44it/s][codecarbon INFO @ 22:11:08] Energy consumed for RAM : 0.000506 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:11:08] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:11:08] 0.002100 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train loss = 0.729247, val loss = 0.729429, time consumed = 0:02:25\n",
      "Model saved after epoch 11\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  32%|███▏      | 9/28 [00:04<00:08,  2.26it/s][codecarbon INFO @ 22:11:23] Energy consumed for RAM : 0.000563 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:11:23] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:11:23] 0.002334 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train loss = 0.726282, val loss = 0.726262, time consumed = 0:02:38\n",
      "Model saved after epoch 12\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:06<00:04,  2.63it/s][codecarbon INFO @ 22:11:38] Energy consumed for RAM : 0.000619 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:11:38] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:11:38] 0.002567 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train loss = 0.722934, val loss = 0.723060, time consumed = 0:02:50\n",
      "Model saved after epoch 13\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:09<00:03,  2.24it/s][codecarbon INFO @ 22:11:53] Energy consumed for RAM : 0.000675 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:11:53] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:11:53] 0.002801 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.29it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train loss = 0.719992, val loss = 0.719829, time consumed = 0:03:03\n",
      "Model saved after epoch 14\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  89%|████████▉ | 25/28 [00:11<00:01,  2.37it/s][codecarbon INFO @ 22:12:08] Energy consumed for RAM : 0.000731 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:12:08] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:12:08] 0.003034 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train loss = 0.716519, val loss = 0.716602, time consumed = 0:03:16\n",
      "Model saved after epoch 15\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train loss = 0.713302, val loss = 0.713389, time consumed = 0:03:28\n",
      "Model saved after epoch 16\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   7%|▋         | 2/28 [00:00<00:10,  2.49it/s][codecarbon INFO @ 22:12:23] Energy consumed for RAM : 0.000788 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:12:23] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:12:23] 0.003267 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.46it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train loss = 0.710234, val loss = 0.710222, time consumed = 0:03:40\n",
      "Model saved after epoch 17\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  36%|███▌      | 10/28 [00:04<00:08,  2.14it/s][codecarbon INFO @ 22:12:38] Energy consumed for RAM : 0.000844 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:12:38] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:12:38] 0.003501 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.42it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train loss = 0.707378, val loss = 0.707109, time consumed = 0:03:52\n",
      "Model saved after epoch 18\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  68%|██████▊   | 19/28 [00:07<00:03,  2.41it/s][codecarbon INFO @ 22:12:53] Energy consumed for RAM : 0.000900 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:12:53] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:12:53] 0.003734 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:12:53] 0.000872 g.CO2eq/s mean an estimation of 27.488781960964953 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:10<00:00,  2.55it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train loss = 0.703661, val loss = 0.704060, time consumed = 0:04:03\n",
      "Model saved after epoch 19\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  86%|████████▌ | 24/28 [00:11<00:01,  2.27it/s][codecarbon INFO @ 22:13:08] Energy consumed for RAM : 0.000956 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:13:08] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:13:08] 0.003968 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train loss = 0.700872, val loss = 0.701092, time consumed = 0:04:16\n",
      "Model saved after epoch 20\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.28it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: train loss = 0.697568, val loss = 0.698208, time consumed = 0:04:29\n",
      "Model saved after epoch 21\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   7%|▋         | 2/28 [00:00<00:09,  2.63it/s][codecarbon INFO @ 22:13:23] Energy consumed for RAM : 0.001013 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:13:23] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:13:23] 0.004201 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: train loss = 0.694850, val loss = 0.695426, time consumed = 0:04:41\n",
      "Model saved after epoch 22\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  25%|██▌       | 7/28 [00:02<00:08,  2.54it/s][codecarbon INFO @ 22:13:38] Energy consumed for RAM : 0.001069 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:13:38] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:13:38] 0.004434 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 27.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: train loss = 0.692243, val loss = 0.692749, time consumed = 0:04:54\n",
      "Model saved after epoch 23\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  43%|████▎     | 12/28 [00:05<00:07,  2.16it/s][codecarbon INFO @ 22:13:53] Energy consumed for RAM : 0.001125 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:13:53] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:13:53] 0.004668 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: train loss = 0.689708, val loss = 0.690182, time consumed = 0:05:07\n",
      "Model saved after epoch 24\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  54%|█████▎    | 15/28 [00:07<00:06,  2.04it/s][codecarbon INFO @ 22:14:08] Energy consumed for RAM : 0.001181 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:14:08] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:14:08] 0.004901 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.14it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: train loss = 0.686905, val loss = 0.687716, time consumed = 0:05:20\n",
      "Model saved after epoch 25\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  71%|███████▏  | 20/28 [00:08<00:03,  2.56it/s][codecarbon INFO @ 22:14:23] Energy consumed for RAM : 0.001238 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:14:23] Energy consumed for all CPUs : 0.003897 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:14:23] 0.005135 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: train loss = 0.684528, val loss = 0.685356, time consumed = 0:05:33\n",
      "Model saved after epoch 26\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  93%|█████████▎| 26/28 [00:11<00:00,  2.32it/s][codecarbon INFO @ 22:14:38] Energy consumed for RAM : 0.001294 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:14:38] Energy consumed for all CPUs : 0.004074 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:14:38] 0.005368 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: train loss = 0.682554, val loss = 0.683099, time consumed = 0:05:45\n",
      "Model saved after epoch 27\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: train loss = 0.680060, val loss = 0.680937, time consumed = 0:05:58\n",
      "Model saved after epoch 28\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  11%|█         | 3/28 [00:01<00:10,  2.45it/s][codecarbon INFO @ 22:14:53] Energy consumed for RAM : 0.001350 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:14:53] Energy consumed for all CPUs : 0.004251 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:14:53] 0.005601 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:14:53] 0.000872 g.CO2eq/s mean an estimation of 27.48860452625517 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: train loss = 0.677935, val loss = 0.678867, time consumed = 0:06:11\n",
      "Model saved after epoch 29\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  25%|██▌       | 7/28 [00:03<00:11,  1.88it/s][codecarbon INFO @ 22:15:08] Energy consumed for RAM : 0.001407 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:15:08] Energy consumed for all CPUs : 0.004428 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:15:08] 0.005835 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: train loss = 0.675884, val loss = 0.676888, time consumed = 0:06:23\n",
      "Model saved after epoch 30\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  50%|█████     | 14/28 [00:06<00:07,  2.00it/s][codecarbon INFO @ 22:15:23] Energy consumed for RAM : 0.001463 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:15:23] Energy consumed for all CPUs : 0.004605 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:15:23] 0.006068 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: train loss = 0.673881, val loss = 0.674990, time consumed = 0:06:36\n",
      "Model saved after epoch 31\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  71%|███████▏  | 20/28 [00:08<00:03,  2.15it/s][codecarbon INFO @ 22:15:38] Energy consumed for RAM : 0.001519 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:15:38] Energy consumed for all CPUs : 0.004782 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:15:38] 0.006301 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: train loss = 0.672069, val loss = 0.673175, time consumed = 0:06:48\n",
      "Model saved after epoch 32\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:10<00:02,  2.14it/s][codecarbon INFO @ 22:15:53] Energy consumed for RAM : 0.001575 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:15:53] Energy consumed for all CPUs : 0.004960 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:15:53] 0.006535 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: train loss = 0.670233, val loss = 0.671434, time consumed = 0:07:01\n",
      "Model saved after epoch 33\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: train loss = 0.668843, val loss = 0.669769, time consumed = 0:07:14\n",
      "Model saved after epoch 34\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   4%|▎         | 1/28 [00:00<00:19,  1.41it/s][codecarbon INFO @ 22:16:08] Energy consumed for RAM : 0.001632 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:16:08] Energy consumed for all CPUs : 0.005137 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:16:08] 0.006768 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: train loss = 0.667047, val loss = 0.668166, time consumed = 0:07:27\n",
      "Model saved after epoch 35\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  21%|██▏       | 6/28 [00:02<00:11,  1.85it/s][codecarbon INFO @ 22:16:23] Energy consumed for RAM : 0.001688 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:16:23] Energy consumed for all CPUs : 0.005314 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:16:23] 0.007002 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: train loss = 0.665785, val loss = 0.666628, time consumed = 0:07:40\n",
      "Model saved after epoch 36\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  36%|███▌      | 10/28 [00:04<00:08,  2.13it/s][codecarbon INFO @ 22:16:38] Energy consumed for RAM : 0.001744 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:16:38] Energy consumed for all CPUs : 0.005491 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:16:38] 0.007235 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.15it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: train loss = 0.663798, val loss = 0.665143, time consumed = 0:07:53\n",
      "Model saved after epoch 37\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  50%|█████     | 14/28 [00:06<00:06,  2.12it/s][codecarbon INFO @ 22:16:53] Energy consumed for RAM : 0.001800 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:16:53] Energy consumed for all CPUs : 0.005668 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:16:53] 0.007468 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:16:53] 0.000872 g.CO2eq/s mean an estimation of 27.48877835421523 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.31it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: train loss = 0.662667, val loss = 0.663715, time consumed = 0:08:05\n",
      "Model saved after epoch 38\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:09<00:02,  2.41it/s][codecarbon INFO @ 22:17:08] Energy consumed for RAM : 0.001857 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:17:08] Energy consumed for all CPUs : 0.005845 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:17:08] 0.007702 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: train loss = 0.660898, val loss = 0.662334, time consumed = 0:08:18\n",
      "Model saved after epoch 39\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  89%|████████▉ | 25/28 [00:11<00:01,  2.31it/s][codecarbon INFO @ 22:17:23] Energy consumed for RAM : 0.001913 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:17:23] Energy consumed for all CPUs : 0.006022 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:17:23] 0.007935 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: train loss = 0.660095, val loss = 0.661004, time consumed = 0:08:31\n",
      "Model saved after epoch 40\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: train loss = 0.658125, val loss = 0.659717, time consumed = 0:08:44\n",
      "Model saved after epoch 41\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   4%|▎         | 1/28 [00:00<00:11,  2.30it/s][codecarbon INFO @ 22:17:38] Energy consumed for RAM : 0.001969 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:17:38] Energy consumed for all CPUs : 0.006200 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:17:38] 0.008169 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: train loss = 0.656929, val loss = 0.658473, time consumed = 0:08:56\n",
      "Model saved after epoch 42\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  25%|██▌       | 7/28 [00:03<00:09,  2.21it/s][codecarbon INFO @ 22:17:53] Energy consumed for RAM : 0.002025 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:17:53] Energy consumed for all CPUs : 0.006377 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:17:53] 0.008402 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.30it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: train loss = 0.655597, val loss = 0.657268, time consumed = 0:09:09\n",
      "Model saved after epoch 43\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  43%|████▎     | 12/28 [00:05<00:06,  2.33it/s][codecarbon INFO @ 22:18:08] Energy consumed for RAM : 0.002082 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:18:08] Energy consumed for all CPUs : 0.006554 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:18:08] 0.008635 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: train loss = 0.654712, val loss = 0.656099, time consumed = 0:09:21\n",
      "Model saved after epoch 44\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  61%|██████    | 17/28 [00:07<00:04,  2.40it/s][codecarbon INFO @ 22:18:23] Energy consumed for RAM : 0.002138 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:18:23] Energy consumed for all CPUs : 0.006731 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:18:23] 0.008869 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: train loss = 0.653974, val loss = 0.654965, time consumed = 0:09:34\n",
      "Model saved after epoch 45\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:10<00:02,  1.97it/s][codecarbon INFO @ 22:18:38] Energy consumed for RAM : 0.002194 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:18:38] Energy consumed for all CPUs : 0.006908 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:18:38] 0.009102 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: train loss = 0.652936, val loss = 0.653860, time consumed = 0:09:47\n",
      "Model saved after epoch 46\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: train loss = 0.651786, val loss = 0.652785, time consumed = 0:10:00\n",
      "Model saved after epoch 47\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   0%|          | 0/28 [00:00<?, ?it/s][codecarbon INFO @ 22:18:53] Energy consumed for RAM : 0.002250 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:18:53] Energy consumed for all CPUs : 0.007085 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:18:53] 0.009336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:18:53] 0.000872 g.CO2eq/s mean an estimation of 27.48890909845976 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.35it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: train loss = 0.650408, val loss = 0.651740, time consumed = 0:10:12\n",
      "Model saved after epoch 48\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:11,  1.93it/s][codecarbon INFO @ 22:19:08] Energy consumed for RAM : 0.002307 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:19:08] Energy consumed for all CPUs : 0.007262 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:19:08] 0.009569 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.29it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: train loss = 0.649383, val loss = 0.650727, time consumed = 0:10:24\n",
      "Model saved after epoch 49\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:05<00:07,  2.42it/s][codecarbon INFO @ 22:19:23] Energy consumed for RAM : 0.002363 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:19:23] Energy consumed for all CPUs : 0.007439 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:19:23] 0.009802 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: train loss = 0.648163, val loss = 0.649734, time consumed = 0:10:37\n",
      "Model saved after epoch 50\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:06<00:05,  2.14it/s][codecarbon INFO @ 22:19:38] Energy consumed for RAM : 0.002419 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:19:38] Energy consumed for all CPUs : 0.007617 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:19:38] 0.010036 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: train loss = 0.647027, val loss = 0.648771, time consumed = 0:10:50\n",
      "Model saved after epoch 51\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  68%|██████▊   | 19/28 [00:09<00:04,  1.84it/s][codecarbon INFO @ 22:19:53] Energy consumed for RAM : 0.002475 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:19:53] Energy consumed for all CPUs : 0.007794 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:19:53] 0.010269 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.17it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: train loss = 0.645953, val loss = 0.647832, time consumed = 0:11:03\n",
      "Model saved after epoch 52\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  89%|████████▉ | 25/28 [00:11<00:01,  2.57it/s][codecarbon INFO @ 22:20:08] Energy consumed for RAM : 0.002532 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:20:08] Energy consumed for all CPUs : 0.007971 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:20:08] 0.010503 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.28it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: train loss = 0.645672, val loss = 0.646911, time consumed = 0:11:16\n",
      "Model saved after epoch 53\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: train loss = 0.644583, val loss = 0.646012, time consumed = 0:11:29\n",
      "Model saved after epoch 54\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   7%|▋         | 2/28 [00:00<00:09,  2.86it/s][codecarbon INFO @ 22:20:23] Energy consumed for RAM : 0.002588 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:20:23] Energy consumed for all CPUs : 0.008148 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:20:23] 0.010736 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: train loss = 0.643273, val loss = 0.645139, time consumed = 0:11:41\n",
      "Model saved after epoch 55\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  25%|██▌       | 7/28 [00:03<00:09,  2.10it/s][codecarbon INFO @ 22:20:38] Energy consumed for RAM : 0.002644 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:20:38] Energy consumed for all CPUs : 0.008325 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:20:38] 0.010969 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: train loss = 0.642749, val loss = 0.644285, time consumed = 0:11:54\n",
      "Model saved after epoch 56\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:05<00:07,  2.16it/s][codecarbon INFO @ 22:20:53] Energy consumed for RAM : 0.002701 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:20:53] Energy consumed for all CPUs : 0.008502 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:20:53] 0.011203 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:20:53] 0.000872 g.CO2eq/s mean an estimation of 27.48892484128766 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: train loss = 0.641985, val loss = 0.643447, time consumed = 0:12:07\n",
      "Model saved after epoch 57\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:07<00:06,  1.92it/s][codecarbon INFO @ 22:21:08] Energy consumed for RAM : 0.002757 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:21:08] Energy consumed for all CPUs : 0.008679 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:21:08] 0.011436 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.09it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 26.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: train loss = 0.641294, val loss = 0.642627, time consumed = 0:12:21\n",
      "Model saved after epoch 58\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  64%|██████▍   | 18/28 [00:09<00:04,  2.13it/s][codecarbon INFO @ 22:21:23] Energy consumed for RAM : 0.002813 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:21:23] Energy consumed for all CPUs : 0.008856 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:21:23] 0.011670 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: train loss = 0.640324, val loss = 0.641828, time consumed = 0:12:34\n",
      "Model saved after epoch 59\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:11<00:04,  1.65it/s][codecarbon INFO @ 22:21:38] Energy consumed for RAM : 0.002869 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:21:38] Energy consumed for all CPUs : 0.009034 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:21:38] 0.011903 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:14<00:00,  1.90it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 26.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: train loss = 0.639489, val loss = 0.641041, time consumed = 0:12:49\n",
      "Model saved after epoch 60\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:11<00:03,  1.98it/s][codecarbon INFO @ 22:21:53] Energy consumed for RAM : 0.002926 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:21:53] Energy consumed for all CPUs : 0.009211 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:21:53] 0.012136 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:14<00:00,  1.96it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 23.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: train loss = 0.638892, val loss = 0.640275, time consumed = 0:13:03\n",
      "Model saved after epoch 61\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  79%|███████▊  | 22/28 [00:11<00:02,  2.36it/s][codecarbon INFO @ 22:22:08] Energy consumed for RAM : 0.002982 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:22:08] Energy consumed for all CPUs : 0.009388 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:22:08] 0.012370 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.05it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: train loss = 0.637668, val loss = 0.639524, time consumed = 0:13:17\n",
      "Model saved after epoch 62\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch:  43%|████▎     | 3/7 [00:00<00:00, 29.38it/s][codecarbon INFO @ 22:22:23] Energy consumed for RAM : 0.003038 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:22:23] Energy consumed for all CPUs : 0.009565 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:22:23] 0.012603 kWh of electricity used since the beginning.\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: train loss = 0.637182, val loss = 0.638792, time consumed = 0:13:30\n",
      "Model saved after epoch 63\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: train loss = 0.636600, val loss = 0.638073, time consumed = 0:13:43\n",
      "Model saved after epoch 64\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  14%|█▍        | 4/28 [00:02<00:13,  1.81it/s][codecarbon INFO @ 22:22:38] Energy consumed for RAM : 0.003094 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:22:38] Energy consumed for all CPUs : 0.009742 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:22:38] 0.012837 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: train loss = 0.636060, val loss = 0.637368, time consumed = 0:13:56\n",
      "Model saved after epoch 65\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  32%|███▏      | 9/28 [00:04<00:08,  2.23it/s][codecarbon INFO @ 22:22:53] Energy consumed for RAM : 0.003151 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:22:53] Energy consumed for all CPUs : 0.009919 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:22:53] 0.013070 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:22:53] 0.000872 g.CO2eq/s mean an estimation of 27.488803200378825 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.06it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: train loss = 0.635042, val loss = 0.636679, time consumed = 0:14:09\n",
      "Model saved after epoch 66\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:05<00:07,  2.42it/s][codecarbon INFO @ 22:23:08] Energy consumed for RAM : 0.003207 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:23:08] Energy consumed for all CPUs : 0.010096 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:23:08] 0.013303 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67: train loss = 0.634255, val loss = 0.636005, time consumed = 0:14:22\n",
      "Model saved after epoch 67\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:07<00:04,  2.45it/s][codecarbon INFO @ 22:23:23] Energy consumed for RAM : 0.003263 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:23:23] Energy consumed for all CPUs : 0.010274 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:23:23] 0.013537 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68: train loss = 0.633899, val loss = 0.635343, time consumed = 0:14:35\n",
      "Model saved after epoch 68\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  64%|██████▍   | 18/28 [00:09<00:04,  2.09it/s][codecarbon INFO @ 22:23:38] Energy consumed for RAM : 0.003319 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:23:38] Energy consumed for all CPUs : 0.010451 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:23:38] 0.013770 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.04it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: train loss = 0.633202, val loss = 0.634695, time consumed = 0:14:49\n",
      "Model saved after epoch 69\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:10<00:02,  2.25it/s][codecarbon INFO @ 22:23:53] Energy consumed for RAM : 0.003376 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:23:53] Energy consumed for all CPUs : 0.010628 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:23:53] 0.014004 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.17it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: train loss = 0.632757, val loss = 0.634063, time consumed = 0:15:02\n",
      "Model saved after epoch 70\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.35it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71: train loss = 0.631936, val loss = 0.633442, time consumed = 0:15:15\n",
      "Model saved after epoch 71\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   0%|          | 0/28 [00:00<?, ?it/s][codecarbon INFO @ 22:24:08] Energy consumed for RAM : 0.003432 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:24:08] Energy consumed for all CPUs : 0.010805 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:24:08] 0.014237 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.47it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 26.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: train loss = 0.631370, val loss = 0.632834, time consumed = 0:15:26\n",
      "Model saved after epoch 72\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  29%|██▊       | 8/28 [00:03<00:07,  2.56it/s][codecarbon INFO @ 22:24:23] Energy consumed for RAM : 0.003488 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:24:23] Energy consumed for all CPUs : 0.010982 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:24:23] 0.014470 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73: train loss = 0.630697, val loss = 0.632242, time consumed = 0:15:39\n",
      "Model saved after epoch 73\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  46%|████▋     | 13/28 [00:05<00:06,  2.47it/s][codecarbon INFO @ 22:24:38] Energy consumed for RAM : 0.003544 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:24:38] Energy consumed for all CPUs : 0.011159 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:24:38] 0.014704 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: train loss = 0.629929, val loss = 0.631664, time consumed = 0:15:52\n",
      "Model saved after epoch 74\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  61%|██████    | 17/28 [00:07<00:04,  2.41it/s][codecarbon INFO @ 22:24:53] Energy consumed for RAM : 0.003601 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:24:53] Energy consumed for all CPUs : 0.011336 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:24:53] 0.014937 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:24:53] 0.000872 g.CO2eq/s mean an estimation of 27.488741711710606 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: train loss = 0.629309, val loss = 0.631096, time consumed = 0:16:04\n",
      "Model saved after epoch 75\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  86%|████████▌ | 24/28 [00:10<00:01,  2.33it/s][codecarbon INFO @ 22:25:08] Energy consumed for RAM : 0.003657 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:25:08] Energy consumed for all CPUs : 0.011513 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:25:08] 0.015170 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: train loss = 0.629227, val loss = 0.630543, time consumed = 0:16:16\n",
      "Model saved after epoch 76\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.18it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77: train loss = 0.628552, val loss = 0.630001, time consumed = 0:16:29\n",
      "Model saved after epoch 77\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   0%|          | 0/28 [00:00<?, ?it/s][codecarbon INFO @ 22:25:23] Energy consumed for RAM : 0.003713 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:25:23] Energy consumed for all CPUs : 0.011691 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:25:23] 0.015404 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: train loss = 0.628002, val loss = 0.629471, time consumed = 0:16:42\n",
      "Model saved after epoch 78\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:09,  2.34it/s][codecarbon INFO @ 22:25:38] Energy consumed for RAM : 0.003770 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:25:38] Energy consumed for all CPUs : 0.011868 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:25:38] 0.015637 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79: train loss = 0.627561, val loss = 0.628957, time consumed = 0:16:55\n",
      "Model saved after epoch 79\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:04<00:08,  2.12it/s][codecarbon INFO @ 22:25:53] Energy consumed for RAM : 0.003826 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:25:53] Energy consumed for all CPUs : 0.012045 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:25:53] 0.015871 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: train loss = 0.627194, val loss = 0.628453, time consumed = 0:17:07\n",
      "Model saved after epoch 80\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:06<00:05,  2.36it/s][codecarbon INFO @ 22:26:08] Energy consumed for RAM : 0.003882 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:26:08] Energy consumed for all CPUs : 0.012222 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:26:08] 0.016104 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.16it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81: train loss = 0.626759, val loss = 0.627961, time consumed = 0:17:21\n",
      "Model saved after epoch 81\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  64%|██████▍   | 18/28 [00:08<00:05,  1.95it/s][codecarbon INFO @ 22:26:23] Energy consumed for RAM : 0.003938 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:26:23] Energy consumed for all CPUs : 0.012399 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:26:23] 0.016337 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: train loss = 0.626118, val loss = 0.627480, time consumed = 0:17:33\n",
      "Model saved after epoch 82\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  89%|████████▉ | 25/28 [00:11<00:01,  2.14it/s][codecarbon INFO @ 22:26:38] Energy consumed for RAM : 0.003995 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:26:38] Energy consumed for all CPUs : 0.012576 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:26:38] 0.016571 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83: train loss = 0.625274, val loss = 0.627011, time consumed = 0:17:46\n",
      "Model saved after epoch 83\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:13<00:02,  1.71it/s][codecarbon INFO @ 22:26:53] Energy consumed for RAM : 0.004051 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:26:53] Energy consumed for all CPUs : 0.012753 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:26:53] 0.016804 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:26:53] 0.000872 g.CO2eq/s mean an estimation of 27.488445022618702 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:15<00:00,  1.83it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84: train loss = 0.625451, val loss = 0.626549, time consumed = 0:18:02\n",
      "Model saved after epoch 84\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  89%|████████▉ | 25/28 [00:12<00:01,  1.98it/s][codecarbon INFO @ 22:27:08] Energy consumed for RAM : 0.004107 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:27:08] Energy consumed for all CPUs : 0.012930 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:27:08] 0.017037 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:14<00:00,  1.95it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 27.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: train loss = 0.625057, val loss = 0.626102, time consumed = 0:18:16\n",
      "Model saved after epoch 85\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.08it/s]\n",
      "Validation batch:   0%|          | 0/7 [00:00<?, ?it/s][codecarbon INFO @ 22:27:23] Energy consumed for RAM : 0.004163 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:27:23] Energy consumed for all CPUs : 0.013108 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:27:23] 0.017271 kWh of electricity used since the beginning.\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86: train loss = 0.624565, val loss = 0.625667, time consumed = 0:18:30\n",
      "Model saved after epoch 86\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.44it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: train loss = 0.623614, val loss = 0.625244, time consumed = 0:18:42\n",
      "Model saved after epoch 87\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:10,  2.12it/s][codecarbon INFO @ 22:27:38] Energy consumed for RAM : 0.004220 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:27:38] Energy consumed for all CPUs : 0.013285 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:27:38] 0.017504 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.45it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: train loss = 0.623542, val loss = 0.624832, time consumed = 0:18:53\n",
      "Model saved after epoch 88\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  46%|████▋     | 13/28 [00:06<00:06,  2.20it/s][codecarbon INFO @ 22:27:53] Energy consumed for RAM : 0.004276 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:27:53] Energy consumed for all CPUs : 0.013462 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:27:53] 0.017738 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:13<00:00,  2.11it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: train loss = 0.623188, val loss = 0.624426, time consumed = 0:19:07\n",
      "Model saved after epoch 89\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  57%|█████▋    | 16/28 [00:07<00:05,  2.19it/s][codecarbon INFO @ 22:28:08] Energy consumed for RAM : 0.004332 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:28:08] Energy consumed for all CPUs : 0.013639 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:28:08] 0.017971 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: train loss = 0.622742, val loss = 0.624035, time consumed = 0:19:20\n",
      "Model saved after epoch 90\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:09<00:03,  2.27it/s][codecarbon INFO @ 22:28:23] Energy consumed for RAM : 0.004388 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:28:23] Energy consumed for all CPUs : 0.013816 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:28:23] 0.018204 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: train loss = 0.622743, val loss = 0.623655, time consumed = 0:19:32\n",
      "Model saved after epoch 91\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  96%|█████████▋| 27/28 [00:12<00:00,  2.13it/s][codecarbon INFO @ 22:28:38] Energy consumed for RAM : 0.004445 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:28:38] Energy consumed for all CPUs : 0.013993 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:28:38] 0.018438 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: train loss = 0.621828, val loss = 0.623285, time consumed = 0:19:45\n",
      "Model saved after epoch 92\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: train loss = 0.621762, val loss = 0.622922, time consumed = 0:19:58\n",
      "Model saved after epoch 93\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:01<00:08,  2.74it/s][codecarbon INFO @ 22:28:53] Energy consumed for RAM : 0.004501 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:28:53] Energy consumed for all CPUs : 0.014170 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:28:53] 0.018671 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:28:53] 0.000872 g.CO2eq/s mean an estimation of 27.48885295766257 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94: train loss = 0.621860, val loss = 0.622570, time consumed = 0:20:11\n",
      "Model saved after epoch 94\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  32%|███▏      | 9/28 [00:03<00:07,  2.39it/s][codecarbon INFO @ 22:29:08] Energy consumed for RAM : 0.004557 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:29:08] Energy consumed for all CPUs : 0.014347 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:29:08] 0.018905 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: train loss = 0.620809, val loss = 0.622231, time consumed = 0:20:23\n",
      "Model saved after epoch 95\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  46%|████▋     | 13/28 [00:06<00:06,  2.15it/s][codecarbon INFO @ 22:29:23] Energy consumed for RAM : 0.004613 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:29:23] Energy consumed for all CPUs : 0.014525 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:29:23] 0.019138 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.19it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: train loss = 0.620558, val loss = 0.621901, time consumed = 0:20:36\n",
      "Model saved after epoch 96\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  68%|██████▊   | 19/28 [00:08<00:03,  2.28it/s][codecarbon INFO @ 22:29:38] Energy consumed for RAM : 0.004670 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:29:38] Energy consumed for all CPUs : 0.014702 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:29:38] 0.019371 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.19it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97: train loss = 0.620444, val loss = 0.621580, time consumed = 0:20:49\n",
      "Model saved after epoch 97\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  75%|███████▌  | 21/28 [00:10<00:03,  2.00it/s][codecarbon INFO @ 22:29:53] Energy consumed for RAM : 0.004726 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:29:53] Energy consumed for all CPUs : 0.014879 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:29:53] 0.019605 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.18it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98: train loss = 0.620315, val loss = 0.621270, time consumed = 0:21:02\n",
      "Model saved after epoch 98\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  93%|█████████▎| 26/28 [00:12<00:00,  2.45it/s][codecarbon INFO @ 22:30:08] Energy consumed for RAM : 0.004782 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:30:08] Energy consumed for all CPUs : 0.015056 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:30:08] 0.019838 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.20it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: train loss = 0.620198, val loss = 0.620968, time consumed = 0:21:15\n",
      "Model saved after epoch 99\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: train loss = 0.619814, val loss = 0.620674, time consumed = 0:21:28\n",
      "Model saved after epoch 100\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  11%|█         | 3/28 [00:01<00:12,  2.00it/s][codecarbon INFO @ 22:30:23] Energy consumed for RAM : 0.004838 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:30:23] Energy consumed for all CPUs : 0.015233 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:30:23] 0.020072 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.19it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101: train loss = 0.618806, val loss = 0.620391, time consumed = 0:21:41\n",
      "Model saved after epoch 101\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  29%|██▊       | 8/28 [00:03<00:08,  2.50it/s][codecarbon INFO @ 22:30:38] Energy consumed for RAM : 0.004895 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:30:38] Energy consumed for all CPUs : 0.015410 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:30:38] 0.020305 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102: train loss = 0.618839, val loss = 0.620117, time consumed = 0:21:54\n",
      "Model saved after epoch 102\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  46%|████▋     | 13/28 [00:05<00:05,  2.75it/s][codecarbon INFO @ 22:30:53] Energy consumed for RAM : 0.004951 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:30:53] Energy consumed for all CPUs : 0.015587 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:30:53] 0.020538 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:30:53] 0.000872 g.CO2eq/s mean an estimation of 27.488641973257728 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103: train loss = 0.619162, val loss = 0.619850, time consumed = 0:22:07\n",
      "Model saved after epoch 103\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  64%|██████▍   | 18/28 [00:08<00:04,  2.41it/s][codecarbon INFO @ 22:31:08] Energy consumed for RAM : 0.005007 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:31:08] Energy consumed for all CPUs : 0.015764 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:31:08] 0.020772 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104: train loss = 0.618528, val loss = 0.619592, time consumed = 0:22:19\n",
      "Model saved after epoch 104\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:10<00:02,  2.28it/s][codecarbon INFO @ 22:31:23] Energy consumed for RAM : 0.005063 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:31:23] Energy consumed for all CPUs : 0.015942 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:31:23] 0.021005 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105: train loss = 0.617647, val loss = 0.619343, time consumed = 0:22:32\n",
      "Model saved after epoch 105\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch:  43%|████▎     | 3/7 [00:00<00:00, 29.58it/s][codecarbon INFO @ 22:31:38] Energy consumed for RAM : 0.005120 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:31:38] Energy consumed for all CPUs : 0.016119 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:31:38] 0.021238 kWh of electricity used since the beginning.\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106: train loss = 0.617854, val loss = 0.619102, time consumed = 0:22:45\n",
      "Model saved after epoch 106\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.19it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107: train loss = 0.617441, val loss = 0.618867, time consumed = 0:22:58\n",
      "Model saved after epoch 107\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  14%|█▍        | 4/28 [00:01<00:11,  2.11it/s][codecarbon INFO @ 22:31:53] Energy consumed for RAM : 0.005176 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:31:53] Energy consumed for all CPUs : 0.016296 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:31:53] 0.021472 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108: train loss = 0.617310, val loss = 0.618639, time consumed = 0:23:11\n",
      "Model saved after epoch 108\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  29%|██▊       | 8/28 [00:03<00:09,  2.13it/s][codecarbon INFO @ 22:32:08] Energy consumed for RAM : 0.005232 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:32:08] Energy consumed for all CPUs : 0.016473 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:32:08] 0.021705 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109: train loss = 0.617405, val loss = 0.618420, time consumed = 0:23:24\n",
      "Model saved after epoch 109\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  43%|████▎     | 12/28 [00:06<00:07,  2.03it/s][codecarbon INFO @ 22:32:23] Energy consumed for RAM : 0.005289 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:32:23] Energy consumed for all CPUs : 0.016650 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:32:23] 0.021939 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.18it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110: train loss = 0.617039, val loss = 0.618208, time consumed = 0:23:37\n",
      "Model saved after epoch 110\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  61%|██████    | 17/28 [00:07<00:05,  2.14it/s][codecarbon INFO @ 22:32:38] Energy consumed for RAM : 0.005345 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:32:38] Energy consumed for all CPUs : 0.016827 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:32:38] 0.022172 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111: train loss = 0.616739, val loss = 0.618002, time consumed = 0:23:50\n",
      "Model saved after epoch 111\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  79%|███████▊  | 22/28 [00:10<00:03,  1.93it/s][codecarbon INFO @ 22:32:53] Energy consumed for RAM : 0.005401 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:32:53] Energy consumed for all CPUs : 0.017004 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:32:53] 0.022405 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:32:53] 0.000872 g.CO2eq/s mean an estimation of 27.48878211568318 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112: train loss = 0.616238, val loss = 0.617805, time consumed = 0:24:03\n",
      "Model saved after epoch 112\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  96%|█████████▋| 27/28 [00:12<00:00,  2.21it/s][codecarbon INFO @ 22:33:08] Energy consumed for RAM : 0.005457 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:33:08] Energy consumed for all CPUs : 0.017181 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:33:08] 0.022639 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113: train loss = 0.616219, val loss = 0.617612, time consumed = 0:24:15\n",
      "Model saved after epoch 113\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114: train loss = 0.616251, val loss = 0.617426, time consumed = 0:24:28\n",
      "Model saved after epoch 114\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  11%|█         | 3/28 [00:01<00:12,  2.01it/s][codecarbon INFO @ 22:33:23] Energy consumed for RAM : 0.005514 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:33:23] Energy consumed for all CPUs : 0.017359 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:33:23] 0.022872 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115: train loss = 0.615768, val loss = 0.617248, time consumed = 0:24:41\n",
      "Model saved after epoch 115\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  29%|██▊       | 8/28 [00:03<00:08,  2.38it/s][codecarbon INFO @ 22:33:38] Energy consumed for RAM : 0.005570 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:33:38] Energy consumed for all CPUs : 0.017536 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:33:38] 0.023106 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.42it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116: train loss = 0.616073, val loss = 0.617075, time consumed = 0:24:53\n",
      "Model saved after epoch 116\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  61%|██████    | 17/28 [00:06<00:04,  2.46it/s][codecarbon INFO @ 22:33:53] Energy consumed for RAM : 0.005626 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:33:53] Energy consumed for all CPUs : 0.017713 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:33:53] 0.023339 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:11<00:00,  2.44it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117: train loss = 0.615337, val loss = 0.616907, time consumed = 0:25:05\n",
      "Model saved after epoch 117\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  82%|████████▏ | 23/28 [00:10<00:02,  2.26it/s][codecarbon INFO @ 22:34:08] Energy consumed for RAM : 0.005682 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:34:08] Energy consumed for all CPUs : 0.017890 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:34:08] 0.023572 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.22it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118: train loss = 0.615670, val loss = 0.616745, time consumed = 0:25:17\n",
      "Model saved after epoch 118\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch:  43%|████▎     | 3/7 [00:00<00:00, 28.93it/s][codecarbon INFO @ 22:34:23] Energy consumed for RAM : 0.005739 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:34:23] Energy consumed for all CPUs : 0.018067 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:34:23] 0.023806 kWh of electricity used since the beginning.\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: train loss = 0.615602, val loss = 0.616589, time consumed = 0:25:30\n",
      "Model saved after epoch 119\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.27it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120: train loss = 0.615871, val loss = 0.616439, time consumed = 0:25:43\n",
      "Model saved after epoch 120\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:10,  2.23it/s][codecarbon INFO @ 22:34:38] Energy consumed for RAM : 0.005795 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:34:38] Energy consumed for all CPUs : 0.018244 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:34:38] 0.024039 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121: train loss = 0.614865, val loss = 0.616293, time consumed = 0:25:55\n",
      "Model saved after epoch 121\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:04<00:08,  2.04it/s][codecarbon INFO @ 22:34:53] Energy consumed for RAM : 0.005851 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:34:53] Energy consumed for all CPUs : 0.018421 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:34:53] 0.024272 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:34:53] 0.000872 g.CO2eq/s mean an estimation of 27.488605763194858 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.30it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122: train loss = 0.614794, val loss = 0.616150, time consumed = 0:26:07\n",
      "Model saved after epoch 122\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  54%|█████▎    | 15/28 [00:07<00:05,  2.26it/s][codecarbon INFO @ 22:35:08] Energy consumed for RAM : 0.005907 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:35:08] Energy consumed for all CPUs : 0.018598 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:35:08] 0.024506 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.33it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123: train loss = 0.614430, val loss = 0.616013, time consumed = 0:26:20\n",
      "Model saved after epoch 123\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  79%|███████▊  | 22/28 [00:09<00:02,  2.05it/s][codecarbon INFO @ 22:35:23] Energy consumed for RAM : 0.005964 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:35:23] Energy consumed for all CPUs : 0.018776 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:35:23] 0.024739 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: train loss = 0.614736, val loss = 0.615881, time consumed = 0:26:32\n",
      "Model saved after epoch 124\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch:  43%|████▎     | 3/7 [00:00<00:00, 29.07it/s][codecarbon INFO @ 22:35:38] Energy consumed for RAM : 0.006020 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:35:38] Energy consumed for all CPUs : 0.018953 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:35:38] 0.024973 kWh of electricity used since the beginning.\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125: train loss = 0.614059, val loss = 0.615752, time consumed = 0:26:45\n",
      "Model saved after epoch 125\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.33it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126: train loss = 0.614509, val loss = 0.615628, time consumed = 0:26:57\n",
      "Model saved after epoch 126\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  21%|██▏       | 6/28 [00:02<00:10,  2.19it/s][codecarbon INFO @ 22:35:53] Energy consumed for RAM : 0.006076 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:35:53] Energy consumed for all CPUs : 0.019130 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:35:53] 0.025206 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127: train loss = 0.614235, val loss = 0.615509, time consumed = 0:27:10\n",
      "Model saved after epoch 127\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  39%|███▉      | 11/28 [00:04<00:06,  2.52it/s][codecarbon INFO @ 22:36:08] Energy consumed for RAM : 0.006132 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:36:08] Energy consumed for all CPUs : 0.019307 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:36:08] 0.025439 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128: train loss = 0.614213, val loss = 0.615393, time consumed = 0:27:23\n",
      "Model saved after epoch 128\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  50%|█████     | 14/28 [00:06<00:07,  1.97it/s][codecarbon INFO @ 22:36:23] Energy consumed for RAM : 0.006189 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:36:23] Energy consumed for all CPUs : 0.019484 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:36:23] 0.025673 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129: train loss = 0.613820, val loss = 0.615282, time consumed = 0:27:35\n",
      "Model saved after epoch 129\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  79%|███████▊  | 22/28 [00:09<00:02,  2.39it/s][codecarbon INFO @ 22:36:38] Energy consumed for RAM : 0.006245 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:36:38] Energy consumed for all CPUs : 0.019661 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:36:38] 0.025906 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130: train loss = 0.613684, val loss = 0.615175, time consumed = 0:27:48\n",
      "Model saved after epoch 130\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  93%|█████████▎| 26/28 [00:11<00:00,  2.24it/s][codecarbon INFO @ 22:36:53] Energy consumed for RAM : 0.006301 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:36:53] Energy consumed for all CPUs : 0.019838 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:36:53] 0.026140 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:36:53] 0.000872 g.CO2eq/s mean an estimation of 27.488529294196475 kg.CO2eq/year\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.28it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131: train loss = 0.613682, val loss = 0.615070, time consumed = 0:28:01\n",
      "Model saved after epoch 131\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.30it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132: train loss = 0.614113, val loss = 0.614970, time consumed = 0:28:13\n",
      "Model saved after epoch 132\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   7%|▋         | 2/28 [00:01<00:17,  1.45it/s][codecarbon INFO @ 22:37:08] Energy consumed for RAM : 0.006357 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:37:08] Energy consumed for all CPUs : 0.020015 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:37:08] 0.026373 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.32it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133: train loss = 0.613432, val loss = 0.614873, time consumed = 0:28:26\n",
      "Model saved after epoch 133\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  32%|███▏      | 9/28 [00:04<00:09,  2.11it/s][codecarbon INFO @ 22:37:23] Energy consumed for RAM : 0.006414 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:37:23] Energy consumed for all CPUs : 0.020193 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:37:23] 0.026606 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.21it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134: train loss = 0.613473, val loss = 0.614778, time consumed = 0:28:38\n",
      "Model saved after epoch 134\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  50%|█████     | 14/28 [00:06<00:06,  2.28it/s][codecarbon INFO @ 22:37:38] Energy consumed for RAM : 0.006470 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:37:38] Energy consumed for all CPUs : 0.020370 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:37:38] 0.026840 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.25it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135: train loss = 0.613905, val loss = 0.614687, time consumed = 0:28:51\n",
      "Model saved after epoch 135\n",
      "Reset patience counter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  68%|██████▊   | 19/28 [00:08<00:04,  2.22it/s][codecarbon INFO @ 22:37:53] Energy consumed for RAM : 0.006526 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:37:53] Energy consumed for all CPUs : 0.020547 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:37:53] 0.027073 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136: train loss = 0.613452, val loss = 0.614598, time consumed = 0:29:04\n",
      "Model saved after epoch 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  86%|████████▌ | 24/28 [00:10<00:01,  2.21it/s][codecarbon INFO @ 22:38:08] Energy consumed for RAM : 0.006582 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:38:08] Energy consumed for all CPUs : 0.020724 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:38:08] 0.027306 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137: train loss = 0.613407, val loss = 0.614512, time consumed = 0:29:17\n",
      "Model saved after epoch 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.26it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138: train loss = 0.613424, val loss = 0.614429, time consumed = 0:29:29\n",
      "Model saved after epoch 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:   4%|▎         | 1/28 [00:00<00:17,  1.57it/s][codecarbon INFO @ 22:38:23] Energy consumed for RAM : 0.006639 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:38:23] Energy consumed for all CPUs : 0.020901 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:38:23] 0.027540 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.23it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139: train loss = 0.612663, val loss = 0.614350, time consumed = 0:29:42\n",
      "Model saved after epoch 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batch\t:  18%|█▊        | 5/28 [00:02<00:11,  2.03it/s][codecarbon INFO @ 22:38:38] Energy consumed for RAM : 0.006695 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:38:38] Energy consumed for all CPUs : 0.021078 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:38:38] 0.027773 kWh of electricity used since the beginning.\n",
      "Training batch\t: 100%|██████████| 28/28 [00:12<00:00,  2.29it/s]\n",
      "Validation batch: 100%|██████████| 7/7 [00:00<00:00, 29.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140: train loss = 0.612757, val loss = 0.614271, time consumed = 0:29:55\n",
      "Model saved after epoch 140\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 22:38:50] Energy consumed for RAM : 0.006739 kWh. RAM Power : 13.5 W\n",
      "[codecarbon INFO @ 22:38:50] Energy consumed for all CPUs : 0.021216 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 22:38:50] 0.027954 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 22:38:50] 0.000872 g.CO2eq/s mean an estimation of 27.488595810342193 kg.CO2eq/year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carbon emission tracking completed. Check 'emissions.csv' for details.\n"
     ]
    }
   ],
   "source": [
    "# Start carbon tracker\n",
    "tracker.start()\n",
    "\n",
    "# train the neural network\n",
    "histories, saved_path = train(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    early_stopping_patience,\n",
    "    lr_scheduler,\n",
    "    saved_path_prefix\n",
    ")\n",
    "\n",
    "import time\n",
    "time.sleep(2)  # Replace this with your actual processing code\n",
    "\n",
    "# Stop tracking after execution\n",
    "tracker.stop()\n",
    "\n",
    "print(\"Carbon emission tracking completed. Check 'emissions.csv' for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the train vs. val loss history\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(histories['train_epoch'])), histories['train_epoch'], '-', label = 'train loss')\n",
    "plt.plot(np.arange(len(histories['val_epoch'])), histories['val_epoch'], '-', label = 'val loss')\n",
    "plt.title('train vs. validation loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBiLSTM(\n",
       "  (embedding): Embedding(24000, 128)\n",
       "  (lstm): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=33, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the best model\n",
    "model.load_state_dict(torch.load(saved_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_dl, model, loss_fn, threshold=0.5):\n",
    "    test_loss = 0.0\n",
    "    all_logits = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in test_dl:\n",
    "            X_batch, y_batch, lengths = X_batch.to(device), y_batch.to(device), lengths.to(device)\n",
    "            \n",
    "            # Forward pass: get predicted probabilities\n",
    "            logits = model(X_batch, lengths)  # Shape: (batch_size, num_classes)\n",
    "            \n",
    "            # Calculate loss for this batch\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predicted probabilities (our model already applies sigmoid)\n",
    "            probs = logits  \n",
    "            \n",
    "            # Convert probabilities to binary predictions using threshold\n",
    "            preds = (probs >= threshold).float()\n",
    "            \n",
    "            # Collect outputs and ground truth for evaluation\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Average test loss over batches\n",
    "    test_loss /= len(test_dl)\n",
    "    \n",
    "    # Concatenate outputs from all batches\n",
    "    all_true = np.concatenate(all_true, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    return all_logits, all_probs, all_preds, all_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6121\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  dark_pigmentation       0.00      0.00      0.00       261\n",
      "               acne       0.00      0.00      0.00       225\n",
      "        eye_contour       0.00      0.00      0.00       149\n",
      "        homogeneity       0.00      0.00      0.00      1063\n",
      "      lack_firmness       0.00      0.00      0.00       292\n",
      "      lack_radiance       0.61      0.93      0.74      1150\n",
      "              pores       0.00      0.00      0.00       311\n",
      "         fine_lines       0.00      0.00      0.00         5\n",
      "wrinkles_fine-lines       0.00      0.00      0.00       657\n",
      "       eye-wrinkles       0.00      0.00      0.00       108\n",
      "      undereye-bags       0.00      0.00      0.00       107\n",
      "            generic       0.00      0.00      0.00        27\n",
      "              18-34       0.91      1.00      0.96      1711\n",
      "              35-54       0.94      1.00      0.97      1765\n",
      "              55-99       0.95      1.00      0.98      1782\n",
      "                dry       0.82      1.00      0.90      1542\n",
      "             normal       0.84      1.00      0.92      1581\n",
      "               oily       0.80      1.00      0.89      1504\n",
      "        combination       0.81      1.00      0.90      1520\n",
      "   sensitivity-high       0.00      0.00      0.00       168\n",
      "    sensitivity-low       0.00      0.00      0.00       276\n",
      "     no_sensitivity       0.83      1.00      0.91      1551\n",
      "               male       0.00      0.00      0.00       724\n",
      "             female       0.90      1.00      0.95      1687\n",
      "            cleanse       0.00      0.00      0.00       354\n",
      "            prepare       0.00      0.00      0.00       130\n",
      "              treat       0.00      0.00      0.00       947\n",
      "           targeted       0.00      0.00      0.00       124\n",
      "               care       0.00      0.00      0.00       301\n",
      "         moisturize       0.00      0.00      0.00       888\n",
      "            protect       0.00      0.00      0.00       373\n",
      "                day       0.90      1.00      0.95      1686\n",
      "              night       0.82      1.00      0.90      1537\n",
      "\n",
      "          micro avg       0.85      0.71      0.78     26506\n",
      "          macro avg       0.31      0.36      0.33     26506\n",
      "       weighted avg       0.61      0.71      0.66     26506\n",
      "        samples avg       0.85      0.72      0.77     26506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Output size = number of labels\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Convert token IDs into embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pack padded sequences for efficient LSTM processing\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, (h, c) = self.lstm(packed_x)\n",
    "        \n",
    "        # Fully connected layer with sigmoid activation (multi-label)\n",
    "        return torch.sigmoid(self.fc(h[-1]))  # Sigmoid for independent label probabilities\n",
    "\n",
    "        Batch size: 64\n",
    "        (val_epoch_loss + 0.00009) < min_val_loss\n",
    "        Time consumed: 9.58 minutes\n",
    "        Epochs needed: N/A (forgot to note)\n",
    "\"\"\"\n",
    "# Evaluate the model on the test set using the test_model function\n",
    "all_logits, all_probs, all_preds, all_true = test_model(test_dl, model, loss_fn, threshold=0.5)\n",
    "\n",
    "# Create a list of output class names from the dataframe (excluding 'text_raw')\n",
    "output_classes = list(df_loreal.columns[1:34])  # Adjust if needed\n",
    "\n",
    "# Print classification metrics for each label\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=output_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6130\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  dark_pigmentation       0.00      0.00      0.00       261\n",
      "               acne       0.00      0.00      0.00       225\n",
      "        eye_contour       0.00      0.00      0.00       149\n",
      "        homogeneity       0.78      0.01      0.01      1063\n",
      "      lack_firmness       0.00      0.00      0.00       292\n",
      "      lack_radiance       0.62      0.99      0.76      1150\n",
      "              pores       0.00      0.00      0.00       311\n",
      "         fine_lines       0.00      0.00      0.00         5\n",
      "wrinkles_fine-lines       0.00      0.00      0.00       657\n",
      "       eye-wrinkles       0.00      0.00      0.00       108\n",
      "      undereye-bags       0.00      0.00      0.00       107\n",
      "            generic       0.00      0.00      0.00        27\n",
      "              18-34       0.91      1.00      0.96      1711\n",
      "              35-54       0.94      1.00      0.97      1765\n",
      "              55-99       0.95      1.00      0.98      1782\n",
      "                dry       0.82      1.00      0.90      1542\n",
      "             normal       0.84      1.00      0.92      1581\n",
      "               oily       0.80      1.00      0.89      1504\n",
      "        combination       0.81      1.00      0.90      1520\n",
      "   sensitivity-high       0.00      0.00      0.00       168\n",
      "    sensitivity-low       0.00      0.00      0.00       276\n",
      "     no_sensitivity       0.83      1.00      0.91      1551\n",
      "               male       0.00      0.00      0.00       724\n",
      "             female       0.90      1.00      0.95      1687\n",
      "            cleanse       0.00      0.00      0.00       354\n",
      "            prepare       0.00      0.00      0.00       130\n",
      "              treat       0.00      0.00      0.00       947\n",
      "           targeted       0.00      0.00      0.00       124\n",
      "               care       0.00      0.00      0.00       301\n",
      "         moisturize       0.00      0.00      0.00       888\n",
      "            protect       0.00      0.00      0.00       373\n",
      "                day       0.90      1.00      0.95      1686\n",
      "              night       0.82      1.00      0.90      1537\n",
      "\n",
      "          micro avg       0.85      0.72      0.78     26506\n",
      "          macro avg       0.33      0.36      0.33     26506\n",
      "       weighted avg       0.64      0.72      0.66     26506\n",
      "        samples avg       0.85      0.72      0.77     26506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Output size = number of labels\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Convert token IDs into embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Pack padded sequences for efficient LSTM processing\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, (h, c) = self.lstm(packed_x)\n",
    "        \n",
    "        # Fully connected layer with sigmoid activation (multi-label)\n",
    "        return torch.sigmoid(self.fc(h[-1]))  # Sigmoid for independent label probabilities\n",
    "\n",
    "        Batch size: 128\n",
    "        (val_epoch_loss + 0.000009) < min_val_loss\n",
    "        Time consumed: 15min\n",
    "        Epochs needed:\n",
    "\"\"\"\n",
    "# Evaluate the model on the test set using the test_model function\n",
    "all_logits, all_probs, all_preds, all_true = test_model(test_dl, model, loss_fn, threshold=0.5)\n",
    "\n",
    "# Create a list of output class names from the dataframe (excluding 'text_raw')\n",
    "output_classes = list(df_loreal.columns[1:34])  # Adjust if needed\n",
    "\n",
    "# Print classification metrics for each label\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=output_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6130\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  dark_pigmentation       0.00      0.00      0.00       261\n",
      "               acne       0.00      0.00      0.00       225\n",
      "        eye_contour       0.00      0.00      0.00       149\n",
      "        homogeneity       0.78      0.01      0.01      1063\n",
      "      lack_firmness       0.00      0.00      0.00       292\n",
      "      lack_radiance       0.62      0.99      0.76      1150\n",
      "              pores       0.00      0.00      0.00       311\n",
      "         fine_lines       0.00      0.00      0.00         5\n",
      "wrinkles_fine-lines       0.00      0.00      0.00       657\n",
      "       eye-wrinkles       0.00      0.00      0.00       108\n",
      "      undereye-bags       0.00      0.00      0.00       107\n",
      "            generic       0.00      0.00      0.00        27\n",
      "              18-34       0.91      1.00      0.96      1711\n",
      "              35-54       0.94      1.00      0.97      1765\n",
      "              55-99       0.95      1.00      0.98      1782\n",
      "                dry       0.82      1.00      0.90      1542\n",
      "             normal       0.84      1.00      0.92      1581\n",
      "               oily       0.80      1.00      0.89      1504\n",
      "        combination       0.81      1.00      0.90      1520\n",
      "   sensitivity-high       0.00      0.00      0.00       168\n",
      "    sensitivity-low       0.00      0.00      0.00       276\n",
      "     no_sensitivity       0.83      1.00      0.91      1551\n",
      "               male       0.00      0.00      0.00       724\n",
      "             female       0.90      1.00      0.95      1687\n",
      "            cleanse       0.00      0.00      0.00       354\n",
      "            prepare       0.00      0.00      0.00       130\n",
      "              treat       0.00      0.00      0.00       947\n",
      "           targeted       0.00      0.00      0.00       124\n",
      "               care       0.00      0.00      0.00       301\n",
      "         moisturize       0.00      0.00      0.00       888\n",
      "            protect       0.00      0.00      0.00       373\n",
      "                day       0.90      1.00      0.95      1686\n",
      "              night       0.82      1.00      0.90      1537\n",
      "\n",
      "          micro avg       0.85      0.72      0.78     26506\n",
      "          macro avg       0.33      0.36      0.33     26506\n",
      "       weighted avg       0.64      0.72      0.66     26506\n",
      "        samples avg       0.85      0.72      0.77     26506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(MultiLabelGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # GRU returns output and hidden state (no cell state)\n",
    "        packed_out, h = self.gru(packed_x)\n",
    "        # Use the last hidden state for classification\n",
    "        return torch.sigmoid(self.fc(h[-1]))\n",
    "\n",
    "        Batch size: 128\n",
    "        (val_epoch_loss + 0.00009) < min_val_loss\n",
    "        Time consumed: \n",
    "        Epochs needed:\n",
    "\"\"\"\n",
    "# Evaluate the model on the test set using the test_model function\n",
    "all_logits, all_probs, all_preds, all_true = test_model(test_dl, model, loss_fn, threshold=0.5)\n",
    "\n",
    "# Create a list of output class names from the dataframe (excluding 'text_raw')\n",
    "output_classes = list(df_loreal.columns[1:34])  # Adjust if needed\n",
    "\n",
    "# Print classification metrics for each label\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=output_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6136\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  dark_pigmentation       0.00      0.00      0.00       255\n",
      "               acne       0.00      0.00      0.00       231\n",
      "        eye_contour       0.00      0.00      0.00       157\n",
      "        homogeneity       0.00      0.00      0.00      1095\n",
      "      lack_firmness       0.00      0.00      0.00       306\n",
      "      lack_radiance       0.63      0.88      0.74      1199\n",
      "              pores       0.00      0.00      0.00       316\n",
      "         fine_lines       0.00      0.00      0.00         2\n",
      "wrinkles_fine-lines       0.00      0.00      0.00       662\n",
      "       eye-wrinkles       0.00      0.00      0.00       121\n",
      "      undereye-bags       0.00      0.00      0.00       114\n",
      "            generic       0.00      0.00      0.00        29\n",
      "              18-34       0.90      1.00      0.95      1691\n",
      "              35-54       0.94      1.00      0.97      1758\n",
      "              55-99       0.95      1.00      0.97      1775\n",
      "                dry       0.83      1.00      0.90      1545\n",
      "             normal       0.84      1.00      0.91      1571\n",
      "               oily       0.81      1.00      0.89      1514\n",
      "        combination       0.82      1.00      0.90      1532\n",
      "   sensitivity-high       0.00      0.00      0.00       172\n",
      "    sensitivity-low       0.00      0.00      0.00       259\n",
      "     no_sensitivity       0.82      1.00      0.90      1533\n",
      "               male       0.00      0.00      0.00       692\n",
      "             female       0.91      1.00      0.95      1703\n",
      "            cleanse       0.00      0.00      0.00       336\n",
      "            prepare       0.00      0.00      0.00       140\n",
      "              treat       0.00      0.00      0.00       992\n",
      "           targeted       0.00      0.00      0.00       123\n",
      "               care       0.00      0.00      0.00       299\n",
      "         moisturize       0.00      0.00      0.00       884\n",
      "            protect       0.00      0.00      0.00       381\n",
      "                day       0.90      1.00      0.95      1677\n",
      "              night       0.81      1.00      0.90      1521\n",
      "\n",
      "          micro avg       0.85      0.71      0.77     26585\n",
      "          macro avg       0.31      0.36      0.33     26585\n",
      "       weighted avg       0.61      0.71      0.66     26585\n",
      "        samples avg       0.85      0.71      0.77     26585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(MultiLabelBiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Bidirectional LSTM: hidden size is doubled after concatenation\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed_x)\n",
    "        # For a single layer bidirectional LSTM, h[-2] is from the forward and h[-1] from the backward pass.\n",
    "        h_forward = h[-2]\n",
    "        h_backward = h[-1]\n",
    "        h_combined = torch.cat((h_forward, h_backward), dim=1)\n",
    "        return torch.sigmoid(self.fc(h_combined))\n",
    "\n",
    "        Batch size: 128\n",
    "        (val_epoch_loss + 0.00009) < min_val_loss\n",
    "        Time consumed: \n",
    "        Epochs needed:\n",
    "\"\"\"\n",
    "# Evaluate the model on the test set using the test_model function\n",
    "all_logits, all_probs, all_preds, all_true = test_model(test_dl, model, loss_fn, threshold=0.5)\n",
    "\n",
    "# Create a list of output class names from the dataframe (excluding 'text_raw')\n",
    "output_classes = list(df_loreal.columns[1:34])  # Adjust if needed\n",
    "\n",
    "# Print classification metrics for each label\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=output_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        Batch size: 128\n",
    "        (val_epoch_loss + 0.00009) < min_val_loss\n",
    "        Time consumed: \n",
    "        Epochs needed:\n",
    "\"\"\"\n",
    "# Evaluate the model on the test set using the test_model function\n",
    "all_logits, all_probs, all_preds, all_true = test_model(test_dl, model, loss_fn, threshold=0.5)\n",
    "\n",
    "# Create a list of output class names from the dataframe (excluding 'text_raw')\n",
    "output_classes = list(df_loreal.columns[1:34])  # Adjust if needed\n",
    "\n",
    "# Print classification metrics for each label\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=output_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
