{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24804bcd-b8f1-47da-b667-a5f6cd97238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.36.24-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.37.0,>=1.36.24 (from boto3)\n",
      "  Using cached botocore-1.36.24-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Using cached s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.37.0,>=1.36.24->boto3)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.37.0,>=1.36.24->boto3)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.24->boto3)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached boto3-1.36.24-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.36.24-py3-none-any.whl (13.4 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.36.24\n",
      "    Uninstalling botocore-1.36.24:\n",
      "      Successfully uninstalled botocore-1.36.24\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.11.2\n",
      "    Uninstalling s3transfer-0.11.2:\n",
      "      Successfully uninstalled s3transfer-0.11.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.36.24\n",
      "    Uninstalling boto3-1.36.24:\n",
      "      Successfully uninstalled boto3-1.36.24\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.36.24 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.36.24 botocore-1.36.24 jmespath-1.0.1 python-dateutil-2.9.0.post0 s3transfer-0.11.2 six-1.17.0 urllib3-2.3.0\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Running boto3 version: 1.36.24\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Initialise \n",
    "!pip install --upgrade --force-reinstall boto3 \n",
    "!pip install openpyxl\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "print('Running boto3 version:', boto3.__version__)\n",
    "\n",
    "def load_excel(file_path, sheet_name=0):\n",
    "    \"\"\"Load data from an Excel file into a Pandas DataFrame.\"\"\"\n",
    "    return pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "excel_data = load_excel('LOREAL_Data.xlsx')\n",
    "print(type(excel_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65877a1-3d40-4e52-a688-725d4f0999b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region:  us-west-2\n"
     ]
    }
   ],
   "source": [
    "# Define Region/Model\n",
    "region = 'us-west-2'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )\n",
    "\n",
    "model_id = \"us.meta.llama3-3-70b-instruct-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9914aa-458f-4c69-b893-9419d318c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "## Model caller\n",
    "def invoke_bedrock_model(client, id, prompt, description, max_tokens=2000, temperature=0.1, top_p=0.2):\n",
    "    response = \"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "            # additionalModelRequestFields={\n",
    "            \n",
    "            # }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Model invocation error\"\n",
    "    try:\n",
    "        result = (response['output']['message']['content'][0]['text']) #\\\n",
    "#        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "#        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "#        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n')\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Output parsing error\"\n",
    "    return result\n",
    "\n",
    "\n",
    "## Call model for description using prompt\n",
    "def run_prompt_for_desc(description):\n",
    "        prompt = (f\"\"\"\n",
    "    ALWAYS use JSON format. You are an expert L'Oreal products with a phd specialised in extracting specific atributes from descriptions. You have been providing the atributes based on a predefined list for a given description for 20 years. Your task is now to read, understand, analyse the following description and extract the atributes requested in the list. You must include ALL the atributes and ALWAYS use the provided options as answers. You Must ALWAYS include the Chain of Thought before providing atributes and at the end you MUST reflect again on your anwser and provide corrections. \n",
    "    \n",
    "    Main keys of the JSON:\n",
    "    0_CoT\n",
    "    1_Skin_Concerns\n",
    "    2_Demographics_Skin_Type\n",
    "    3_Product_Function_Routine\n",
    "    4_Time_of_use\n",
    "    \n",
    "    The anwser should follow the structure Key:atribute:presence. Here is an exemple: \n",
    "    \n",
    "    Description: \n",
    "    ($135 Value) iS Clinical Active Serum, 1 Oz Our most popular   product, this fast-acting iS Clinical Active Serum, long-term,   results-oriented formula decreases the appearance of fine lines and wrinkles,   evens skin tone, and is excellent for acne. Touted by physicians as   remarkable and phenomenal, ACTIVE SERUM usually produces results within a   couple of days. Potent activity will be noted upon application as evidenced   by a cool tingling sensation indicating penetration. Excellent for all skin   types and for all ages, this powerful botanical serum does not dry the skin.   Instead, it leaves the skin moist and smooth. Reduces appearance of fine lines and wrinkles Smooths and softens skin Helps acne-prone skin\n",
    "    \n",
    "    Response: \n",
    "    0. Chain of Though\n",
    "    For Skin Concerns:\n",
    "    Identifying Key Phrases:\n",
    "    The description mentions “decreases the appearance of fine lines and wrinkles,” which indicates issues related to aging.\n",
    "    “Evens skin tone” suggests an improvement in overall skin homogeneity.\n",
    "    “Excellent for acne” and “helps acne-prone skin” clearly point to acne being addressed.\n",
    "    Mapping to Attributes:\n",
    "    Since there’s no mention of dark pigmentation, eye contour concerns, issues with pores, or under-eye issues, those are marked as “No.”\n",
    "    Although “fine lines” are mentioned, the provided interpretation groups them under a combined “wrinkles_fine-lines” attribute, so “fine_lines” is marked “No” while “wrinkles_fine-lines” is marked “Yes.”\n",
    "    “Lack radiance” is interpreted as being improved by the product (i.e., the product works on dullness), so it is marked “Yes.”\n",
    "    For Demographics & Skin Type:\n",
    "    Age & Skin Types:\n",
    "    The phrase “excellent for all ages” supports marking 18–34, 35–54, and 55–99 as “Yes.”\n",
    "    “Excellent for all skin types” covers dry, normal, oily, and combination.\n",
    "    Sensitivity & Gender:\n",
    "    “Does not dry the skin” and the absence of any warning suggest no sensitivity issues (thus “no_sensitivity” is Yes, while sensitivity levels are No).\n",
    "    Although “all ages” might imply unisex use, the interpretation here follows the sample outcome by marking “female” as Yes and “male” as No.\n",
    "    For Product Function & Routine:\n",
    "    \n",
    "    Purpose of the Product:\n",
    "    Being an “active serum” designed to reduce fine lines, wrinkles, and help with acne indicates its role is treatment.\n",
    "    There’s no mention of cleansing, prepping, moisturizing, or protecting functions; hence, only “treat” is marked “Yes.”\n",
    "    For Time of Use:\n",
    "    \n",
    "    Usage Indications:\n",
    "    Even though the description doesn’t state a specific time, active serums are commonly used both in the morning and at night for optimal benefits.\n",
    "    Thus, both “day” and “night” are marked “Yes.”\n",
    "    \n",
    "    \n",
    "    1_Skin Concerns\n",
    "    dark_pigmentation: Yes\n",
    "    acne: Yes\n",
    "    eye_contour: No\n",
    "    homogeneity: Yes\n",
    "    lack_firmness: No\n",
    "    lack_radiance: Yes\n",
    "    pores: No\n",
    "    fine_lines: No\n",
    "    wrinkles_fine-lines: Yes\n",
    "    eye-wrinkles: No\n",
    "    undereye-bags: No\n",
    "    2_Demographics & Skin Type\n",
    "    generic: No\n",
    "    18-34: Yes\n",
    "    35-54: Yes\n",
    "    55-99: Yes\n",
    "    dry: Yes\n",
    "    normal: Yes\n",
    "    oily: Yes\n",
    "    combination: Yes\n",
    "    sensitivity-high: No\n",
    "    sensitivity-low: No\n",
    "    no_sensitivity: Yes\n",
    "    male: No\n",
    "    female: Yes\n",
    "    3_Product Function & Routine\n",
    "    cleanse: No\n",
    "    prepare: No\n",
    "    treat: Yes\n",
    "    targeted: No\n",
    "    care: No\n",
    "    moisturize: No\n",
    "    protect: No\n",
    "    4_Time of use\n",
    "    day: Yes\n",
    "    night: Yes\n",
    "    \n",
    "    Here is the current description you have to extract atributes from: \"{description}\".\n",
    "    ALWAYS use JSON format.\n",
    "    \"\"\")\n",
    "        #print(prompt)\n",
    "        response = invoke_bedrock_model(bedrock, model_id, prompt, description)\n",
    "        return response\n",
    "\n",
    "\n",
    "# For Loop that returns a list of responses from the LLM\n",
    "def loop_for_data_and_range(source, num_rows=1, start_point=0):\n",
    "    responses = []\n",
    "    step = 0\n",
    "    for row in range(num_rows):\n",
    "        description = source[row + start_point]\n",
    "        response = run_prompt_for_desc(description)\n",
    "        responses.append(response)\n",
    "        print(step)\n",
    "        step = step + 1\n",
    "    return responses\n",
    "\n",
    "def extract_df(response):  \n",
    "    attributes = [\n",
    "    \"dark_pigmentation\",\n",
    "    \"acne\",\n",
    "    \"eye_contour\",\n",
    "    \"homogeneity\",\n",
    "    \"lack_firmness\",\n",
    "    \"lack_radiance\",\n",
    "    \"pores\",\n",
    "    \"fine_lines\",\n",
    "    \"wrinkles_fine-lines\",\n",
    "    \"eye-wrinkles\",\n",
    "    \"undereye-bags\",\n",
    "    \"generic\",\n",
    "    \"18-34\",\n",
    "    \"35-54\",\n",
    "    \"55-99\",\n",
    "    \"dry\",\n",
    "    \"normal\",\n",
    "    \"oily\",\n",
    "    \"combination\",\n",
    "    \"sensitivity-high\",\n",
    "    \"sensitivity-low\",\n",
    "    \"no_sensitivity\",\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"cleanse\",\n",
    "    \"prepare\",\n",
    "    \"treat\",\n",
    "    \"targeted\",\n",
    "    \"care\",\n",
    "    \"moisturize\",\n",
    "    \"protect\",\n",
    "    \"day\",\n",
    "    \"night\"\n",
    "    ]\n",
    "    # Create the data dictionary starting with the description\n",
    "    data = {}\n",
    "    \n",
    "    # Loop through attributes and set the value to 1 if Yes, 0 if No, or None if not found.\n",
    "    for attribute in attributes:\n",
    "        if f'\"{attribute}\": \"Yes\"' in response:\n",
    "            data[attribute] = 1\n",
    "        elif f'\"{attribute}\": \"No\"' in response:\n",
    "            data[attribute] = 0\n",
    "        else:\n",
    "            data[attribute] = None\n",
    "    \n",
    "    # Specify the column order so that 'description' comes first\n",
    "    columns = attributes\n",
    "    \n",
    "    # Create the DataFrame with a single row\n",
    "    df = pd.DataFrame([data], columns=columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def loop_extract(responses):\n",
    "    dfs = []\n",
    "    for response in responses: \n",
    "        extracted_resp = extract_df(response)\n",
    "        dfs.append(extracted_resp)\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e156e1-7e33-4af7-a329-a824a538ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "Saved file: final_excel_from_0_to_750(1).xlsx\n"
     ]
    }
   ],
   "source": [
    "attributes = [\n",
    "    \"dark_pigmentation\",\n",
    "    \"acne\",\n",
    "    \"eye_contour\",\n",
    "    \"homogeneity\",\n",
    "    \"lack_firmness\",\n",
    "    \"lack_radiance\",\n",
    "    \"pores\",\n",
    "    \"fine_lines\",\n",
    "    \"wrinkles_fine-lines\",\n",
    "    \"eye-wrinkles\",\n",
    "    \"undereye-bags\",\n",
    "    \"generic\",\n",
    "    \"18-34\",\n",
    "    \"35-54\",\n",
    "    \"55-99\",\n",
    "    \"dry\",\n",
    "    \"normal\",\n",
    "    \"oily\",\n",
    "    \"combination\",\n",
    "    \"sensitivity-high\",\n",
    "    \"sensitivity-low\",\n",
    "    \"no_sensitivity\",\n",
    "    \"male\",\n",
    "    \"female\",\n",
    "    \"cleanse\",\n",
    "    \"prepare\",\n",
    "    \"treat\",\n",
    "    \"targeted\",\n",
    "    \"care\",\n",
    "    \"moisturize\",\n",
    "    \"protect\",\n",
    "    \"day\",\n",
    "    \"night\"\n",
    "    ]\n",
    "num_rows=750\n",
    "# First row is 0\n",
    "start_point=0\n",
    "\n",
    "responses = loop_for_data_and_range(source=excel_data['text_raw'], num_rows=num_rows, start_point=start_point)\n",
    "columns = attributes\n",
    "final_df = pd.DataFrame(columns=columns)\n",
    "final_df = loop_extract(responses)\n",
    "# Here, we're assuming that excel_data['text_raw'] is a pandas Series.\n",
    "descriptions = excel_data['text_raw'].iloc[start_point : start_point + num_rows]\n",
    "\n",
    "# Insert the description column as the first column in the final DataFrame\n",
    "final_df.insert(0, 'description', descriptions.values)\n",
    "\n",
    "# Create a filename using f-string formatting\n",
    "file_name = f\"final_excel_from_{start_point}_to_{start_point + num_rows}(1).xlsx\"\n",
    "\n",
    "# Save the DataFrame to an Excel file in the current directory\n",
    "final_df.to_excel(file_name, index=False)\n",
    "\n",
    "print(f\"Saved file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70be23-ff3f-46d3-9a19-ca186f2ff099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def majority_vote(paths=None, folder_path=None, output_file=\"final_majority_vote.xlsx\"):\n",
    "    \"\"\"\n",
    "    Performs majority voting on multiple Excel files containing multi-label classification results.\n",
    "    Assumes the first column contains non-numeric descriptions that are kept unchanged.\n",
    "    \n",
    "    Parameters:\n",
    "    - paths: List of file paths to Excel files.\n",
    "    - folder_path: Path to a folder containing Excel files.\n",
    "    - output_file: Filename for the final majority voted results.\n",
    "    \n",
    "    Returns:\n",
    "    - Saves the final Excel file with majority voted results.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    \n",
    "    # Use provided file paths\n",
    "    if paths is not None:\n",
    "        file_list.extend(paths)\n",
    "    \n",
    "    # If folder_path is provided, add all .xlsx files from that folder\n",
    "    if folder_path is not None:\n",
    "        folder_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "        file_list.extend(folder_files)\n",
    "    \n",
    "    # Remove duplicates in case both options were used and overlap\n",
    "    file_list = list(set(file_list))\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(\"No Excel files found. Provide a valid list of paths or folder path.\")\n",
    "    \n",
    "    # List to store numeric parts of DataFrames (columns 2 to end)\n",
    "    dfs_numeric = []\n",
    "    descriptions = None  # To store the descriptions from the first column\n",
    "    \n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_excel(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Separate the first column (descriptions)\n",
    "        if descriptions is None:\n",
    "            # Keep the first column as is. It will be attached later.\n",
    "            descriptions = df.iloc[:, [0]]\n",
    "        # If descriptions already exists, you might want to check consistency here.\n",
    "        \n",
    "        # Process numeric columns: columns 2 to end\n",
    "        df_numeric = df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "        # Replace NaN (empty cells or non-numeric cells) with 0.5 (neutral value)\n",
    "        df_numeric = df_numeric.fillna(0.5)\n",
    "        dfs_numeric.append(df_numeric)\n",
    "    \n",
    "    if not dfs_numeric:\n",
    "        raise ValueError(\"No valid DataFrames loaded from the provided files.\")\n",
    "    \n",
    "    # Stack all numeric parts into a 3D NumPy array. Assumes all DataFrames have the same shape.\n",
    "    arr = np.array([df.values for df in dfs_numeric], dtype=float)\n",
    "    \n",
    "    # Compute the average for each cell across all runs\n",
    "    avg = np.mean(arr, axis=0)\n",
    "    \n",
    "    # Majority vote: if average > 0.5 then 1, else 0.\n",
    "    majority_vote_arr = (avg > 0.5).astype(int)\n",
    "    \n",
    "    # Create a DataFrame for the numeric results using the columns and index from the first numeric DataFrame\n",
    "    final_numeric_df = pd.DataFrame(majority_vote_arr, columns=dfs_numeric[0].columns, index=dfs_numeric[0].index)\n",
    "    \n",
    "    # Combine the descriptions (first column) with the majority voted numeric results\n",
    "    final_df = pd.concat([descriptions, final_numeric_df], axis=1)\n",
    "    \n",
    "    # Save the final DataFrame to a new Excel file\n",
    "    final_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"Final majority voted Excel file saved as '{output_file}'.\")\n",
    "\n",
    "path = \"/Users/sebastian/Documents/Jup for run/Excels L'Oreal/final_excel_from_0_to_750\"\n",
    "majority_vote(folder_path=path)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31e2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
